{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick an short tutorial to setup example backend on lxplus\n",
    "\n",
    "This is based on the Nivida example tutorial for Pytorch backend develment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Pytorch model\n",
    "\n",
    "```bash\n",
    "cd ~\n",
    "mkdir TritonDemo\n",
    "#  Clone the Official tutorial \n",
    "git clone https://github.com/triton-inference-server/tutorials.git . \n",
    "# Cache directory\n",
    "SINGULARITY_CACHEDIR=\"/eos/user/{INITIAL}/{YOUR_ACCOUNT}/singularity/\"\n",
    "# image folder, beeter to store in EOS\n",
    "export IMAGE_FOLDER=\"/eos/user/{INITIAL}/{YOUR_ACCOUNT}/TritonDemo/\"\n",
    "#Pull the image\n",
    "singularity pull --dir $IMAGE_FOLDER docker://nvcr.io/nvidia/pytorch:22.04-py3\n",
    "# Run the image\n",
    "singularity run --nv -B /afs -B /eos -B /cvmfs pytorch_22.04-py3.sif\n",
    "\n",
    "# Get the model.pt\n",
    "python export.py\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the model configs and structure \n",
    "```\n",
    "model_repository\n",
    "|\n",
    "+-- resnet50\n",
    "    |\n",
    "    +-- config.pbtxt\n",
    "    +-- 1\n",
    "        |\n",
    "        +-- model.pt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Triton Inference Server\n",
    "\n",
    "```bash \n",
    "singularity run --nv -e --no-home -B {YOUR_MODEL_FOLDER}:/models tritonserver_22.04-py3.sif\n",
    "\n",
    "tritonserver --model-repository=/models\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Client \n",
    "\n",
    "Open another terminal to run the\n",
    "\n",
    "Using the LXPLUS-GPU, we need to make sure using the same machine as the one with Server to avoid the need to deal with authentication.  \n",
    "\n",
    "```bash \n",
    "# Longin to same LXPLUS-GPU\n",
    "\n",
    "ssh \n",
    "\n",
    "\n",
    "export IMAGE_FOLDER=\"/eos/user/{initial}/{whoami}/TritonDemo/\"\n",
    "\n",
    "singularity --dir $IMAGE_FOLDER  pull docker:/nvcr.io/nvidia/tritonserver:22.04-py3-sdk\n",
    "\n",
    "\n",
    "singularity run --nv -e  -B /cvmfs:/cvmfs -B /afs/cern.ch/user/{initial}:/home -B /afs/cern.ch/user/{initial}/{whoami}:/srv -B /afs:/afs -B /eos:/eos tritonserver_22.04-py3-sdk.sif\n",
    "\n",
    "# Check if the connection is ok \n",
    "curl -v localhost:8000/v2/health/ready\n",
    "\n",
    "You should see the following message if the connect and ther server is in good state\n",
    "\n",
    "< HTTP/1.1 200 OK\n",
    "< Content-Length: 0\n",
    "< Content-Type: text/plain\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter serves as an introduction to get started with Triton.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Official tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server\n",
    "\n",
    "- [Quickstart Tutorial](https://github.com/triton-inference-server/server/blob/main/docs/getting_started/quickstart.md)\n",
    "- [Conceptual Guide](https://github.com/triton-inference-server/tutorials/blob/main/Conceptual_Guide/Part_1-model_deployment/README.md)\n",
    "- [Quick Deployment](https://github.com/triton-inference-server/tutorials?tab=readme-ov-file#quick-deploy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backend Development\n",
    "\n",
    "- [Triton Example Backends](https://github.com/triton-inference-server/backend/tree/main/examples)    \n",
    "   Three examples backends are provided to demonstrate how to develop a custom backend for Triton. \n",
    "   - Minimal Triton Backend\n",
    "   - Recommended Triton Backend\n",
    "   - BLS Triton Backend\n",
    "\n",
    "   A straightforward exercise to get started is to write a simple backend that receive variable-length input and returns the same input as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "[NVIDIA Triton Inference Server](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upcoming tutorial \n",
    "[Inner Detector Tracking Workshop 2024](https://indico.cern.ch/event/1374927/)   \n",
    "\n",
    "[US ATLAS Summer Workshop 2024](https://indico.cern.ch/event/1348862/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
